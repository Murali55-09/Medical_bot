{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bae993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Hugging Face cache (G drive)\n",
    "os.environ[\"HF_HOME\"] = \"G:/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:/hf_cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"G:/hf_cache/datasets\"\n",
    "\n",
    "# Reduce memory pressure\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce85abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325e7cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Medical_bot\\medical-pathology-qlora\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Medical_bot\\medical-pathology-qlora\\.venv\\lib\\site-packages\\transformers\\utils\\hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8031c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ec0dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa7ed05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # GPT-Neo attention layers\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccdc3a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 1,317,148,672 || trainable%: 0.1194\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "566a0cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"data/general_pathology.jsonl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daa1500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(example):\n",
    "    prompt = (\n",
    "        f\"### Instruction:\\n{example['instruction']}\\n\\n\"\n",
    "        f\"### Question:\\n{example['input']}\\n\\n\"\n",
    "        f\"### Answer:\\n{example['output']}\"\n",
    "    )\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "    # ðŸ”‘ THIS LINE FIXES THE ERROR\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f00f37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = dataset.map(\n",
    "    format_data,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a397e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8120871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.462600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12, training_loss=4.443501710891724, metrics={'train_runtime': 40.9622, 'train_samples_per_second': 1.831, 'train_steps_per_second': 0.293, 'total_flos': 278790458572800.0, 'train_loss': 4.443501710891724, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0986fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crypto is a type of encryption that uses a key to encrypt data.\n",
      "\n",
      "Cryptography is the art of encoding information in such a way that it can be decoded by a third party.\n",
      "Cryptology is the study of cryptography.\n",
      "The term \"cryptology\" is used to describe the study and use of cryptography in the field of computer security.\n",
      "A cryptographic system is a system\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "prompt = \"\"\"You are a medical pathology assistant.\n",
    "Answer ONLY pathology-related questions.\n",
    "If the question is not related to pathology, say:\n",
    "\"I can only answer pathology-related questions.\"\n",
    "\n",
    "Question:\n",
    "What is crypto?\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "output = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=80,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.2,\n",
    "    no_repeat_ngram_size=3,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "print(output[0][\"generated_text\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d44c4f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./pathology_lora_adapters\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1344e969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./pathology_lora_adapters\\\\tokenizer_config.json',\n",
       " './pathology_lora_adapters\\\\special_tokens_map.json',\n",
       " './pathology_lora_adapters\\\\vocab.json',\n",
       " './pathology_lora_adapters\\\\merges.txt',\n",
       " './pathology_lora_adapters\\\\added_tokens.json',\n",
       " './pathology_lora_adapters\\\\tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94599d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
